{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d8795-d021-41e2-9ee8-e7ac40142adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these separately to install the required libraries\n",
    "!pip install hdbscan\n",
    "!pip install basemap\n",
    "!pip install folium\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f58871-7c2e-4c32-b1ed-531f83bdc682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244981a-db6d-40e7-8807-443cf57c286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textual Metadata Processing\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# Specify directory and convert to CSV\n",
    "import pandas as pd\n",
    "location=r\"Results\\Input Records\\Filtered1M.csv\"\n",
    "data = pd.read_csv(location, header=None)\n",
    "data.to_csv(r\"Results\\TextualMetadataProcessing\\TP1M.csv\",columns=[0,1,3,5,12,13,14],index=False,header=['LineNo','PhotoID','UserID','DateTaken','Latitude','Longitude','Accuracy'])\n",
    "\n",
    "# Change file name\n",
    "location=r\"Results\\Input Records\\Filtered1M.csv\"\n",
    "dest = r\"Results\\TextualMetadataProcessing\\TP1M.csv\"\n",
    "data = pd.read_csv(location, header=None)\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z_]+')\n",
    "data2 = pd.read_csv(dest\n",
    "                   )\n",
    "data2['Tags'] = \"\"\n",
    "Info=[]\n",
    "tagPid=defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a92f3-6155-44e4-a31e-8c6adde9331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Selecting appropriate metadata\n",
    "for i in range(0,len(data.index)):\n",
    "    user_tag=str(data[10][i])\n",
    "    user_tag+=str(data[8][i])\n",
    "    user_tag+=str(data[9][i])\n",
    "    filtered = []\n",
    "    filtered = list(set(filtered + tokenizer.tokenize(user_tag)))\n",
    "\n",
    "\n",
    "    # removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_removed =[]\n",
    "    for w in filtered:\n",
    "        if w not in stop_words:\n",
    "            stop_removed.append(w)\n",
    "\n",
    "    # reducing words to their root form\n",
    "    stemmed=[]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in stop_removed:\n",
    "        stemmed.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    # obtain only nouns\n",
    "    nouns = []\n",
    "    for w in stemmed:\n",
    "        for word,pos in nltk.pos_tag([w]):\n",
    "             if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(w)\n",
    "\n",
    "    #Appending words in root form to file\n",
    "    k=\"\"\n",
    "    final = list(set(nouns))\n",
    "    for g in final:\n",
    "        k+=g\n",
    "        if g in tagPid.keys():\n",
    "            tagPid[g].append(data[1][i])\n",
    "        else:\n",
    "            tagPid[g].append(data[1][i])\n",
    "        k+=\" \"\n",
    "    Info+=final\n",
    "    \n",
    "    data2[\"Tags\"][i]=k\n",
    "Info=list(set(Info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f29fe-b36c-4d3b-9a1f-f3cf53ba3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data2.to_csv(\"Results/Input records/Textproc1M.csv\",index=False)\n",
    "newfile= \"Results/Input records/Textproc1M.csv\"\n",
    "newframe = pd.read_csv(newfile)\n",
    "\n",
    "#Binary vector to find which all tags have been used\n",
    "bv = np.zeros((26710, len(Info)), dtype=int)\n",
    "\n",
    "int_tags={}\n",
    "for w in Info:\n",
    "    int_tags[w]=0\n",
    "tags=newframe.Tags\n",
    "for s in tags:\n",
    "    i = 0\n",
    "    l = s.split(\" \")\n",
    "    count = 0\n",
    "    for w in Info:\n",
    "        if w in l:\n",
    "            count += 1\n",
    "            bv[i][Info.index(w)] = 1\n",
    "        int_tags[w] += 1\n",
    "        i += 1\n",
    "\n",
    "            \n",
    "#Filtering tags which appear more than 1 time\n",
    "i=0\n",
    "count=0\n",
    "for i in int_tags.keys():\n",
    "    if int_tags[i] > 1:\n",
    "        count+=1\n",
    "#Finding the interesting words based on threshold\n",
    "interested={}\n",
    "threshold = 1/len(newframe)\n",
    "for w in Info:\n",
    "\n",
    "    if int_tags[w]/len(newframe) > threshold:\n",
    "        interested[w] = int_tags[w]/len(Info)\n",
    "#Finding photo ID corresponding to interesting words\n",
    "photo_id=[]\n",
    "for w in interested.keys():\n",
    "    photo_id=list(set(photo_id+tagPid[w]))\n",
    "df=newframe.query('PhotoID in @photo_id')  \n",
    "df = newframe.query('PhotoID in @photo_id')\n",
    "output_path = r\"Results/TextualMetadataProcessing\"  # Use raw string or escape backslashes\n",
    "TP_op1M = 'TP_op1M.csv'\n",
    "file_path = os.path.join(output_path, TP_op1M)\n",
    "df.to_csv(file_path, columns=['PhotoID', 'UserID', 'Latitude', 'Longitude', 'Accuracy'], index=False, header=['PhotoID', 'UserID', 'Latitude', 'Longitude', 'Accuracy'])\n",
    "\n",
    "print('TP_op1M csv file is created for further processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35001c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographical Data Clustering\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import hdbscan\n",
    "import folium\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# Loading CSV File (Output of Text processing step)\n",
    "input_file = \"Results/TextualMetadataProcessing/TP_op1M.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "df1 = df.copy()\n",
    "coord = {}\n",
    "Lat = np.asarray(df['Latitude'])\n",
    "Long = np.asarray(df['Longitude'])\n",
    "coords = df[['Longitude', 'Latitude']].values\n",
    "rads = np.radians(coords)\n",
    "\n",
    "\n",
    "# Clustering step\n",
    "clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0,metric='haversine', gen_min_span_tree=True, min_cluster_size=20, min_samples=None, p=None).fit(rads)\n",
    "cluster_labels = clusterer.labels_\n",
    "num_clusters = len(set(cluster_labels))\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(-1, num_clusters)])\n",
    "cluster_label=(clusterer.labels_)\n",
    "x = cluster_label\n",
    "cluster_points={}\n",
    "for i in range(num_clusters + 1):\n",
    "    cluster_points[i] = []\n",
    "    \n",
    "cluster_points[-1]=[]\n",
    "\n",
    "for i in range(0,len(x)):\n",
    "    l = cluster_points[x[i]]\n",
    "    l.append(coords[i])\n",
    "    cluster_points[x[i]] = l\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plotting points on map\n",
    "m = Basemap(projection='merc', resolution='l', epsg=4269, llcrnrlon=-122.567288, llcrnrlat=37.696627, urcrnrlon=-122.329308, urcrnrlat=37.852144)\n",
    "x, y = m(coords[:, 1], coords[:, 0])\n",
    "m.scatter(x, y, 5, marker='o', color='b')\n",
    "m.arcgisimage(service='World_Shaded_Relief', xpixels=5000, verbose=False)\n",
    "plt.show()\n",
    "\n",
    "def get_cmap(N):\n",
    "    '''\n",
    "    Returns a function that maps each index in 0, 1, ... N-1 to a distinct \n",
    "    RGB color.\n",
    "    '''\n",
    "    color_norm = colors.Normalize(vmin=0, vmax=N-1)\n",
    "    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='nipy_spectral')\n",
    "    def map_index_to_rgb_color(index):\n",
    "        return scalar_map.to_rgba(index)\n",
    "    return map_index_to_rgb_color\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "m = Basemap(projection='merc', resolution='l', epsg=4269, llcrnrlon=-122.567288, llcrnrlat=37.696627, urcrnrlon=-122.329308, urcrnrlat=37.852144)\n",
    "\n",
    "unique_label = np.unique(cluster_labels)\n",
    "cmaps = get_cmap(num_clusters)\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    lons_select = cluster[:, 1]\n",
    "    lats_select = cluster[:, 0]\n",
    "    x, y = m(lons_select, lats_select)\n",
    "    m.scatter(x, y, 5, marker='o', color=cmaps(i), zorder=10)\n",
    "\n",
    "m.arcgisimage(service='World_Shaded_Relief', xpixels=5000, verbose=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fccee-6f59-4124-9074-be338461aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend Analysis\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "regr = lm.LinearRegression()\n",
    "clf = Lasso(alpha=0.1)\n",
    "\n",
    "# Loading CSV corresponding to a particular region\n",
    "df = pd.read_csv(r\"Results/TrendEstimation/fullaustralia.csv\", header=None)\n",
    "df2 = pd.to_datetime(df[5])\n",
    "months = list(range(1, 13))\n",
    "years = list(range(2004, 2014))\n",
    "df[5] = pd.to_datetime(df[5])\n",
    "ct = []\n",
    "\n",
    "X_my = []\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        l = [month, year]\n",
    "        X_my.append(l)\n",
    "        ct.append(len(df.loc[operator.and_(df[5].dt.year == year, df[5].dt.month == month)]))\n",
    "\n",
    "df[5] = pd.to_datetime(df[5])\n",
    "ct_test = []\n",
    "X_my_test = []\n",
    "year = 2014\n",
    "for month in months:\n",
    "    l = [month, year]\n",
    "    X_my_test.append(l)\n",
    "    ct_test.append(len(df.loc[operator.and_(df[5].dt.year == year, df[5].dt.month == month)]))\n",
    "\n",
    "X_my = np.asarray(X_my)\n",
    "Ct = np.asarray(ct).reshape(-1, 1)\n",
    "X_my_test = np.asarray(X_my_test)\n",
    "Ct_test = np.asarray(ct_test).reshape(-1, 1)\n",
    "\n",
    "# NORMALIZING MONTH-YEAR VALUE\n",
    "nry = []\n",
    "for i in X_my:\n",
    "    g = i[0]\n",
    "    k = i[1]\n",
    "    norm = k + (round((g / 13), 2))\n",
    "    nry.append(norm)\n",
    "\n",
    "nry_test = []\n",
    "for i in X_my_test:\n",
    "    g = i[0]\n",
    "    k = i[1]\n",
    "    norm = k + (round((g / 13), 2))\n",
    "    nry_test.append(norm)\n",
    "\n",
    "# Linear - deg 1 and 2\n",
    "poly = make_pipeline(PolynomialFeatures(1), regr)\n",
    "\n",
    "# nry vs no of tourists\n",
    "# Trying out various combinations\n",
    "\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(3), regr)\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(1), regr)\n",
    "\n",
    "# X_my vs No of tourists\n",
    "\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(3), regr)\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "# Ridge - deg 1 and 2\n",
    "poly = make_pipeline(PolynomialFeatures(1), Ridge())\n",
    "\n",
    "# nry vs no of tourists\n",
    "\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(3), Ridge())\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(1), Ridge())\n",
    "\n",
    "# X_my vs No of tourists\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(3), Ridge())\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "# Lasso - deg 1 and 2\n",
    "poly = make_pipeline(PolynomialFeatures(1), clf)\n",
    "\n",
    "# nry vs no of tourists\n",
    "\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(3), clf)\n",
    "poly.fit(np.asarray(nry).reshape(-1, 1), Ct)\n",
    "Y_pred = poly.predict(np.asarray(nry).reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "\n",
    "# X_my vs No of tourists\n",
    "\n",
    "poly = make_pipeline(PolynomialFeatures(1), clf)\n",
    "\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred.reshape(-1, 1))\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n",
    "poly = make_pipeline(PolynomialFeatures(3), clf)\n",
    "\n",
    "poly.fit(X_my, Ct)\n",
    "Y_pred = poly.predict(X_my)\n",
    "plt.plot(np.asarray(nry), Y_pred)\n",
    "plt.plot(np.asarray(nry), Ct)\n",
    "mean_absolute_error(Ct, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7da018-cce5-4221-8574-9d9fd2af571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "%matplotlib inline\n",
    "import calendar\n",
    "import sklearn\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "# Opening CSV File corresponding to a particular region\n",
    "location =\"Results/Input records/Filtered1M.csv\"\n",
    "df = pd.read_csv(location, header=None)\n",
    "df2 = pd.to_datetime(df[5])\n",
    "months = list(range(1, 13))\n",
    "df[5] = pd.to_datetime(df[5])\n",
    "ct = []\n",
    "\n",
    "# Finding data wrt to each Month\n",
    "for i in months:\n",
    "    ct.append(len(df.loc[(df[5].dt.month == i)]))\n",
    "for i in range(len(ct)):\n",
    "    ct[i] = ct[i] / 10\n",
    "\n",
    "months = np.asarray(months)\n",
    "count = np.asarray(ct)\n",
    "plt.scatter(months, count)\n",
    "plt.plot(months, count, color='blue', linewidth=1)\n",
    "months = list(range(1, 13))\n",
    "year = list(range(2004, 2014))\n",
    "allmonths = {}\n",
    "\n",
    "for i in months:\n",
    "    allmonths[i] = []\n",
    "for i in year:\n",
    "    for j in months:\n",
    "        allmonths[j].append(len(df.loc[operator.and_(df[5].dt.year == i, df[5].dt.month == j)]))\n",
    "\n",
    "# Checking value for April\n",
    "regr = lm.LinearRegression()\n",
    "sc = []\n",
    "\n",
    "# Plotting data for each month to find Trend for that particular month\n",
    "\n",
    "for month in months:\n",
    "    monthCt = np.asarray(allmonths[month])\n",
    "    yr = np.asarray(year)\n",
    "\n",
    "    plt.scatter(year, monthCt)\n",
    "\n",
    "    poly = make_pipeline(PolynomialFeatures(3), regr)\n",
    "\n",
    "    poly.fit(yr.reshape(-1, 1), monthCt)\n",
    "\n",
    "    Y_pred = poly.predict(yr.reshape(-1, 1).reshape(-1, 1))\n",
    "\n",
    "    plt.plot(year, monthCt, color=\"orange\", linewidth=1)\n",
    "    plt.plot(year, Y_pred.reshape(-1, 1), color='blue', linewidth=1)\n",
    "\n",
    "    month_diff = []\n",
    "    for i in range(len(monthCt)):\n",
    "        month_diff.append(abs(monthCt[i] - Y_pred[i]))\n",
    "\n",
    "    sc.append(mean_absolute_error(monthCt, Y_pred))\n",
    "\n",
    "    plt.title(\"Month: {}\".format(month))\n",
    "    mean_absolute_error(monthCt, Y_pred)\n",
    "\n",
    "# Plot of Months with average no. of visitors\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "month_average_diff = [np.mean(diff) if isinstance(diff, list) else diff for diff in month_diff]\n",
    "\n",
    "for month in range(1, 13):\n",
    "    if month <= len(month_diff) and isinstance(month_diff[month-1], list):\n",
    "        ax.scatter(month, np.asarray(month_diff[month-1]))\n",
    "    elif month <= len(month_diff):\n",
    "        ax.scatter(month, month_diff[month-1])\n",
    "    \n",
    "ax.scatter(months, np.asarray(sc))\n",
    "ax.plot(months, np.asarray(sc), color='blue', linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"Average count of tourists\")\n",
    "\n",
    "ax.plot(months, np.asarray(sc), color='blue', linewidth=3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(months, np.asarray(sc))\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"Seasonal Component\")\n",
    "ax.plot(months, np.asarray(sc), color='blue', linewidth=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
